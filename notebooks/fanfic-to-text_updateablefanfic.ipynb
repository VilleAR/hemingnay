{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements: python >= 3.7\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# these too\n",
    "import sqlite3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = [\n",
    "    \"metadata112.sqlite\",\n",
    "    \"metadata113.sqlite\",\n",
    "    \"metadata114.sqlite\",\n",
    "    \"metadata115.sqlite\",\n",
    "    \"metadata116.sqlite\",\n",
    "    \"metadata117.sqlite\",\n",
    "    \"metadata118.sqlite\",\n",
    "    \"metadata119.sqlite\",\n",
    "    \"metadata120.sqlite\",\n",
    "    \"metadata121.sqlite\",\n",
    "]\n",
    "\n",
    "data_path = Path('/mnt/e/datasets/fanfic/updateablefanfic')\n",
    "categories_path = Path.cwd().joinpath('updateablefanfic_categories.json')\n",
    "\n",
    "records_output_path = Path.cwd().joinpath('../data/records.json')\n",
    "dataset_output_path = Path.cwd().joinpath('../data/dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "records = []\n",
    "\n",
    "def query(md, conn, cat):\n",
    "    result = None\n",
    "    try:\n",
    "        curs = conn.cursor()\n",
    "        cat = cat.replace(\"'\", \"''\")\n",
    "\n",
    "        # Selects stories within specified categories, max 8 stories per category,\n",
    "        # with between 100 to 15000 words. No poetry.\n",
    "        sqlite_select_query = \"\"\"\n",
    "            SELECT path \n",
    "            FROM metadata \n",
    "            WHERE language LIKE 'english' \n",
    "                AND CAST(REPLACE(words, ',', '') AS int) > 100 \n",
    "                AND CAST(REPLACE(words, ',', '') AS int) < 15000 \n",
    "                AND genre NOT LIKE '%Poetry%'\n",
    "                AND category LIKE '%\"\"\" + cat + \"\"\"%' \n",
    "            ORDER BY CAST(REPLACE(words, ',', '') AS int) ASC\n",
    "            LIMIT 8;\"\"\"\n",
    "        # and cast(chapters as integer) < 2\n",
    "\n",
    "        curs.execute(sqlite_select_query)\n",
    "        \n",
    "        return curs.fetchall()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def get_records():\n",
    "    records = []\n",
    "    with open(categories_path, encoding=\"utf8\") as fcats:\n",
    "        cats = json.load(fcats)\n",
    "        max_i = len(cats) * len(metadata) + len(metadata * 500)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers = 64) as executor:\n",
    "            futures = []\n",
    "\n",
    "            # Progress bar\n",
    "            with tqdm(total=max_i) as pbar:\n",
    "                for md in metadata:\n",
    "                    # Load the DBs into memory. \n",
    "                    source = sqlite3.connect(data_path.joinpath(md))\n",
    "                    conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
    "                    source.backup(conn)\n",
    "                    pbar.update(500)\n",
    "\n",
    "                    for cat in cats:\n",
    "                        cat = cat.replace(\"'\", \"''\")\n",
    "                        future = executor.submit(query, md, conn, cat)\n",
    "                        futures.append(future)\n",
    "\n",
    "                output = []\n",
    "\n",
    "                \n",
    "                for f in as_completed(futures):\n",
    "                    records += f.result()\n",
    "                    pbar.update(1)\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_records(records):\n",
    "    with open(records_output_path, \"w+\", encoding=\"utf8\") as rfile:\n",
    "        json.dump(records, rfile)\n",
    "\n",
    "def load_records():\n",
    "    with open(records_output_path, \"r\", encoding=\"utf8\") as rfile:\n",
    "        return json.load(rfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10400/10400 [04:59<00:00, 34.68it/s]\n"
    }
   ],
   "source": [
    "records = get_records()\n",
    "\n",
    "write_records(records)"
   ]
  },
  {
   "source": [
    "def parse_file(file_path):\n",
    "    try:\n",
    "        parsed = \"\"\n",
    "        quote_re = r'^\".*\"'\n",
    "\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "            data=file.read()\n",
    "            metadata_skipped = False\n",
    "\n",
    "            for l in iter(data.splitlines()):\n",
    "                # Skip first few rows of genre information etc.\n",
    "                if not metadata_skipped:\n",
    "                    if l.startswith(\"Summary:\"):\n",
    "                        metadata_skipped = True\n",
    "\n",
    "                else:\n",
    "                    if not l.startswith(\"\\t\"):\n",
    "                        l = l.strip()\n",
    "                        if (not l == 'End file.') and len(l):\n",
    "                            if \".\" in l or \"?\" in l or \"!\" in l:\n",
    "                                # Weeding out lines with no written words, e.g. \"---\"\n",
    "                                if len(l) > 10 or re.search(quote_re, l):\n",
    "                                    l = re.sub(\n",
    "                                        r\"[^A-Za-z0-9,.?!\\-—():; '\\\"\\*]+\", '', l)\n",
    "\n",
    "                                    # Remove some characters appearing multiple times in a row\n",
    "                                    # Maybe leave them in for extra spice?\n",
    "                                    l = re.sub(r\"[—]{2,}\", '—', l)\n",
    "                                    l = re.sub(r\"[,]{2,}\", ',', l)\n",
    "                                    l = re.sub(r\"[/]{2,}\", '/', l)\n",
    "                                    l = re.sub(r\"[-]{2,}\", '-', l)\n",
    "                                    l = re.sub(r\"[']{2,}\", \"'\", l)\n",
    "                                    l = re.sub(r'[\"]{2,}', '\"', l)\n",
    "                                    l = re.sub(r'[(]{2,}', '(', l)\n",
    "                                    l = re.sub(r'[)]{2,}', ')', l)\n",
    "\n",
    "                                    parsed += l + \"\\n\"\n",
    "\n",
    "        return parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    records=load_records()\n",
    "\n",
    "    with open(dataset_output_path, \"a+\", newline=\"\", encoding=\"utf8\") as output_file:\n",
    "        # Try lowering workers if it gets stuck\n",
    "        with ThreadPoolExecutor(max_workers = 1024) as executor:\n",
    "            # Progress bar\n",
    "            with tqdm(total=len(records)) as pbar:\n",
    "                futures = []\n",
    "\n",
    "                for r in records:\n",
    "                    future = executor.submit(parse_file, data_path.joinpath(r[0]))\n",
    "                    futures.append(future)\n",
    "\n",
    "                for f in as_completed(futures):\n",
    "                    # Append start and end tokens for GPT-2\n",
    "                    text = '<|startoftext|>\\n' + f.result() + '<|endoftext|>\\n'\n",
    "                    output_file.write(text)\n",
    "\n",
    "                    pbar.update(1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 27069/27069 [00:51<00:00, 529.61it/s]\n"
    }
   ],
   "source": [
    "create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}